I used NTLK and Vader for this assignment. I looked at reviews for BYU on Google Maps. Using my own judgement, I identified 7 reviews with an overall positive sentiment, and 7 with negative sentiment. Most of the reviews were clearly either positive or negative, but some were a little harder too define. For example, some reviews gave pros and cons of the university, so I had to decide where it fell on the spectrum overall. Before running the analysis, I guessed the sentiment scores for each text sample. After I had done that, I ran each sample through the Vader analyzer, and printed the results. As seen in [vader.png](vader.png), the text file names indicate my score guesses. The format for the file names is `<pos/neg>_<id>_<myScoreGuess>.txt`. For example, my guess for file neg_1_8.txt would be -0.8. 

The output of my Vader script ([vader.png](vader.png)) shows the actual sentiment scores for each file. Based on my judgement, 12 out of the 14 files were identified correctly in terms of direction. As for the magnitude of the scores, Vader did pretty well, and to be honest, I trust Vader's score more than mine on some of these files. Two files, however, were identified with the wrong sign. Neg_6_7.txt, which was scored as extremely positive, because it contains phrases like "nice and clean", "pleasure", and "pleasant." It seems to me that the review used positive language to convey a negative message about BYU, saying things like "BYU is not a great school", and urging people to go to UVU instead. Opposite of this, was pos_7_5.txt, which was given a negative score, despite being an overall positive review in my opinion. Although the review talks about BYU's safe environment and value, it recounts an interaction with administration what "was so frustrating", so I wasn't too surprised that Vader rated it negatively.

For the most part, I think Vader did a good job at classifying reviews. Reviews on Google Maps have a "social media feel" to them, so I'd imagine Vader was a good choice, since the docs highlight Vader's capabilities in interpretting social media text. 

I also implemented the code given in the NLTK/Vader tutorial, and built a ML model with NLTK's NaiveBayesClassifier and SentimentAnalyzer. I split the data into 5 files for training and 2 for testing (for each category), built the model, then evaluated it against the testing set. The classifier had an overal accuracy of 75% for one iteration, and 100% for another. Using Vader, the "accuracy" was 86% (12/14), which is close to the average of 75% and 100%. I know that's not a very scientific comparison, but it was still interesting to run classic sentiment analysis with a tool like Vader, as well as using a ML model to group documents based on their sentiment.