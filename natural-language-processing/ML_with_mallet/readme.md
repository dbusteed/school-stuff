For this assignment I used MALLET workbench. I've used lots of machine learning tools in the past, including Azure ML Studio, scikit-learn, and JMP, but this was my first time using MALLET workbench. It took me a while before I understood exactly how the tool worked. I was mostly confused on how to format the input files correctly so that the ML algorithms could distinguish features from labels. After I reread the docs several times, I figured it out. 

I wanted to build a ML model that would do classification on scripts from Seinfeld episodes. Specifically, I would get every line that the four main characters (Jerry, George, Kramer, Elaine) said, and see if the ML model could correctly classify who said what lines. I downloaded a corpus from [https://github.com/luonglearnstocode/Seinfeld-text-corpus](https://github.com/luonglearnstocode/Seinfeld-text-corpus), which contains the script for every Seinfeld episode. For the corpus to work with MALLET, I wrote a [Python script](split_lines_by_character.py) that read thru each line of the script, then put each line of dialog into a seperate folder for each character. When the script finished, I had a folder for each character, with each folder containing a seperate text file for each line that was said in the show. 

I then used the 'import-dir' MALLET command to create a 'mallet' file, which could be loaded in the classifier models. One cool feature of MALLET is the ability to train/test multiple models in one command. I did a 70/30 split on the data, the ran it through a Maximum Entropy, Naive Bayes Classifer, Decision Tree, and Winnow model (see [train_cmd.png](train_cmd.png) for the command I ran). It took less than a minute to run three seperate trials, and I was given a comprehensive set of results when it finished (see [raw_output.png](raw_output.png)). I put these results into a spreadsheet for visualization purposes (see [results.png](results.png)). As seen in the graphs, the models did an okay job. Without the ML models, we would have a 25% chance of guessing a character's line. The MaxEnt and NaiveBayes models had precision scores around 40%, which indicates that the models aren't useless, although it'd be a stretch to say that they performed well. I think this is partly due the fact that many lines are extremely short, in which it is harded to run tests used in document classification.

I was interested in seeing similar document classification in action, but this time with real documents instead of one-liners from a TV show. I collected eight Wikipedia articles, four about politicians, and four about musicians. I know that n=8 for a machine learning model is kinda silly, but I had already spent ~2 hours on this assignment at this point, and thought that this small sample would be fine for my curiousity's sake. I placed the files in a similar manner as before, built the .mallet file, then ran it through the NaiveBayes classifier (this time splitting 50/50). The model was 100% accurate in both recall and precision, which was cool to see, even though I know this wasn't a very practical example (see [matrix.png](matrix.png) for a confusion matrix given during testing).

I found that MALLET workbench was a really cool tool to use for document classification. Plus, learning new tools is always a fun thing to do.